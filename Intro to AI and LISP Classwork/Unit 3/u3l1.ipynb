{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([ [5,10,1], [5,6,0], [-4,0,1], [-5,-6,0], [4,8,1], [0,0,0], [-6,4,1], [4,0,0], [2,-5,0], [4,-5,0], [-4,-3,0] ]).astype(float)\n",
    "x = np.c_[np.ones(len(data)), data[:, [0,1]]]\n",
    "y = data[:, 2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thewa\\AppData\\Local\\Temp\\ipykernel_18040\\3007688375.py:23: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = np.sum(np.dot(y, np.log(pred)) + np.dot(1-y, np.log(1-pred)))\n"
     ]
    }
   ],
   "source": [
    "# Starting Weights\n",
    "w = [0, 0, 0]\n",
    "\n",
    "# Bashed through lots of trial and error\n",
    "lr = 0.2\n",
    "\n",
    "# Cost Function Graphing\n",
    "costx = []\n",
    "costy = []\n",
    "\n",
    "# Flag for loop\n",
    "flag = True\n",
    "\n",
    "# Iteration countter\n",
    "c = 0\n",
    "\n",
    "# Gradient Descent Loop (Expected time: 15 sec)\n",
    "while flag:\n",
    "    c += \n",
    "    \n",
    "    # hw(X)\n",
    "    pred = np.divide(1, 1+np.exp(-1 * np.dot(x, w)))\n",
    "    diff = pred - y\n",
    "    grad = np.dot(diff, x)/len(x)\n",
    "    \n",
    "    # cost function\n",
    "    cost = np.sum(np.dot(y, np.log(pred)) + np.dot(1-y, np.log(1-pred)))\n",
    "    \n",
    "    # updating\n",
    "    w = w - lr * grad\n",
    "    \n",
    "    costx.append(c)\n",
    "    costy.append(cost)\n",
    "    \n",
    "    # break condition\n",
    "    if c > 100000 or np.linalg.norm(grad) < 0.0001:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-12.28661975,  -5.12159376,   5.11232109])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
